# ğŸ§¥ AI Virtual Try-On

> Production-grade SaaS virtual try-on platform powered by **Stable Diffusion XL + ControlNet**.
> Upload a person photo and a clothing image â€” get a photorealistic 1024px try-on result in under 15 seconds.

![Architecture](docs/architecture.md)

---

## âœ¨ Features

- **SDXL + ControlNet** â€“ pose-conditioned inpainting for photorealistic results
- **Segformer B2 Clothes** â€“ accurate human parsing and clothing mask extraction
- **TPS Clothing Warp** â€“ affine + thin-plate-spline geometric fitting
- **Poisson Blending** â€“ seamless boundary compositing + face identity preservation
- **FP16 + xFormers** â€“ memory-efficient inference on 24GB+ GPUs
- **Async Queue** â€“ Celery + Redis for non-blocking, scalable inference
- **S3-Compatible Storage** â€“ AWS S3, MinIO, Cloudflare R2
- **Modern UI** â€“ glassmorphism design, drag-and-drop, before/after slider

---

## ğŸ“ Project Structure

```
tryon/
â”œâ”€â”€ app/                   # FastAPI backend
â”‚   â”œâ”€â”€ main.py            # App entry point + lifespan model loading
â”‚   â”œâ”€â”€ config.py          # Pydantic Settings (env-driven)
â”‚   â”œâ”€â”€ models/loader.py   # Model preloader (SDXL, ControlNet, Segformer, OpenPose)
â”‚   â”œâ”€â”€ pipeline/          # 5-stage ML pipeline
â”‚   â”‚   â”œâ”€â”€ segmentation.py
â”‚   â”‚   â”œâ”€â”€ pose.py
â”‚   â”‚   â”œâ”€â”€ warping.py
â”‚   â”‚   â”œâ”€â”€ inpainting.py
â”‚   â”‚   â””â”€â”€ blending.py
â”‚   â”œâ”€â”€ routers/           # API endpoints
â”‚   â”œâ”€â”€ queue/             # Celery worker + tasks
â”‚   â”œâ”€â”€ storage/           # S3 adapter
â”‚   â””â”€â”€ utils/             # Image utilities
â”œâ”€â”€ frontend/              # Vanilla HTML/CSS/JS SPA
â”œâ”€â”€ nginx/                 # Reverse proxy config
â”œâ”€â”€ scripts/               # Startup scripts
â”œâ”€â”€ docs/                  # Architecture, API, optimization notes
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env.example
```

---

## ğŸš€ Quick Start

### Prerequisites
- Docker + Docker Compose v2
- NVIDIA GPU with â‰¥24GB VRAM + NVIDIA Container Toolkit
- ~20GB disk space (model weights)

### 1. Clone and configure

```bash
git clone <your-repo> tryon
cd tryon
cp .env.example .env
# Edit .env â€” add S3 credentials if desired
```

### 2. Launch

```bash
docker compose up --build
```

> âš ï¸ **First run:** models are downloaded from Hugging Face (~15GB). This takes 20â€“40 minutes. Subsequent starts load from the `model_cache` volume in ~90 seconds.

### 3. Open the UI

```
http://localhost
```

API Docs: `http://localhost/docs`

---

## â˜ï¸ Deployment

### AWS EC2 (g5.xlarge â€“ A10G 24GB)

```bash
# 1. Launch g5.xlarge with Deep Learning AMI (Ubuntu 22.04)
# 2. Install Docker + NVIDIA Container Toolkit
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker ubuntu

# NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-ct.gpg
echo "deb [signed-by=/usr/share/keyrings/nvidia-ct.gpg] https://nvidia.github.io/libnvidia-container/stable/deb/$(. /etc/os-release; echo $ID$VERSION_ID) /" | sudo tee /etc/apt/sources.list.d/nvidia-ct.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# 3. Deploy
git clone <your-repo> tryon && cd tryon
cp .env.example .env   # fill in S3 credentials
docker compose up -d --build
```

### RunPod

1. Create a pod with **NVIDIA A4000/A6000**, runtime image: `nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04`
2. Open HTTP port 80
3. Clone repo, configure `.env`, run `docker compose up --build`

---

## âš™ï¸ Configuration

See `.env.example` for all options. Key variables:

| Variable | Default | Description |
|---|---|---|
| `DEVICE` | `cuda` | `cuda` or `cpu` |
| `USE_FP16` | `true` | Enable FP16 precision |
| `USE_XFORMERS` | `true` | Enable xFormers attention |
| `NUM_INFERENCE_STEPS` | `30` | Diffusion steps (20=fast, 50=best) |
| `OUTPUT_SIZE` | `1024` | Output image size in pixels |
| `S3_BUCKET` | `tryon-results` | S3 bucket for results |
| `REDIS_URL` | `redis://redis:6379/0` | Celery broker URL |

---

## ğŸ“– Documentation

| Doc | Description |
|---|---|
| [Architecture](docs/architecture.md) | System overview + Mermaid diagrams |
| [API Reference](docs/api.md) | Endpoint docs + curl / Python examples |
| [Model Optimization](docs/model_optimization.md) | FP16, xFormers, VAE tiling, batching |

---

## ğŸ“Š Performance Targets

| GPU | Inference Time | Quality |
|---|---|---|
| A10G (24GB) | ~10-13s | âœ… Production |
| A100 (40GB) | ~6-9s | âœ… Best |
| RTX 4090 (24GB) | ~8-11s | âœ… Production |
| RTX 3090 (24GB) | ~12-15s | âœ… Acceptable |

---

## ğŸ“œ License

MIT â€” feel free to use for commercial and non-commercial projects.
